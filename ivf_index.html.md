# IVF Index: Making Vector Search Fast


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

## Distance Functions: How We Measure Similarity

To find similar vectors, we need ways to measure how “close” or “far
apart” they are. Different distance functions capture different types of
similarity:

------------------------------------------------------------------------

<a
href="https://github.com/anubhavmaity/MiniVecDB/blob/main/minivecdb/ivf_index.py#L17"
target="_blank" style="float:right; font-size:smaller">source</a>

### vers

>  vers (a, b)

<table>
<thead>
<tr>
<th></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>a</td>
<td></td>
</tr>
<tr>
<td>b</td>
<td>1 - cosine</td>
</tr>
</tbody>
</table>

``` python
v1 = np.array([1, 0, 0])       
v2 = np.array([0, 1, 0])
v3 = np.array([-0.5, -0.5, 0])

query = np.array([0.7, 0.7, 0]) 
vers(v1, query), vers(v2, query), vers(v3, query)
```

    (np.float64(0.2928932259563096),
     np.float64(0.2928932259563096),
     np.float64(1.9999999857142858))

------------------------------------------------------------------------

<a
href="https://github.com/anubhavmaity/MiniVecDB/blob/main/minivecdb/ivf_index.py#L20"
target="_blank" style="float:right; font-size:smaller">source</a>

### euclidean

>  euclidean (a, b)

``` python
euclidean(v1, query), euclidean(v2, query), euclidean(v3, query)
```

    (np.float64(0.7615773105863908),
     np.float64(0.7615773105863908),
     np.float64(1.697056274847714))

------------------------------------------------------------------------

<a
href="https://github.com/anubhavmaity/MiniVecDB/blob/main/minivecdb/ivf_index.py#L23"
target="_blank" style="float:right; font-size:smaller">source</a>

### neg_dot

>  neg_dot (a, b)

<table>
<thead>
<tr>
<th></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>a</td>
<td></td>
</tr>
<tr>
<td>b</td>
<td>negative dot</td>
</tr>
</tbody>
</table>

``` python
neg_dot(v1, query), neg_dot(v2, query), neg_dot(v3, query)
```

    (np.float64(-0.7), np.float64(-0.7), np.float64(0.7))

Different distance functions are better for different tasks:

- **Cosine**: Best for semantic similarity when vector size doesn’t
  matter
- **Euclidean**: Best when absolute positions in vector space matter
- **Dot Product**: Good for normalized vectors (all same length)

Our examples show how these measures rank similarity differently for the
same vectors.

## Understanding K-means Clustering

K-means clustering is an algorithm that groups similar data points by
iteratively assigning points to the nearest of K centers, then updating
those centers based on the assigned points. The FastKMeans library we’re
using optimizes this process through efficient distance calculations and
smarter initialization, which helps our IVF Index group similar vectors
into clusters – allowing us to search just a few promising clusters
rather than the entire database, dramatically improving search speed as
our collection grows.

------------------------------------------------------------------------

<a
href="https://github.com/anubhavmaity/MiniVecDB/blob/main/minivecdb/ivf_index.py#L33"
target="_blank" style="float:right; font-size:smaller">source</a>

### run_kmeans

>  run_kmeans (data)

``` python
centroids, preds = run_kmeans(np.random.randn(20, 768))
centroids.shape, preds.shape
```

    ((3, 768), (20,))

## Building the Index

The key to the IVF approach is dividing vectors into clusters. We’ll use
K-means clustering, which:

1.  Randomly selects initial cluster centers
2.  Assigns each vector to the nearest center
3.  Recalculates centers based on assigned vectors
4.  Repeats until convergence

Our
[`build_index`](https://anubhavmaity.github.io/MiniVecDB/ivf_index.html#build_index)
function handles this process:

------------------------------------------------------------------------

<a
href="https://github.com/anubhavmaity/MiniVecDB/blob/main/minivecdb/ivf_index.py#L39"
target="_blank" style="float:right; font-size:smaller">source</a>

### build_index

>  build_index (vs)

``` python
vs = VectorStorage.load('data/headline_embeddings.json')
```

``` python
cluster_ids_map, id_cluster_map, centroids = build_index(vs)
```

## The IVFIndex Class

Now we’ll build a complete index that: 1. Stores vectors and cluster
assignments 2. Lets us search efficiently 3. Can add new vectors

The key parameters are: - `nprobe`: How many closest clusters to search
(higher = more accurate but slower) - `distance_fn`: Which distance
metric to use (cosine, euclidean, or dot)

## Using the Index

Let’s try our index on a real dataset of headline embeddings:

1.  Then we build the index from our storage(clustering the vectors)
2.  Finally, we search for similar headlines

The stats show us how vectors are distributed across clusters.

------------------------------------------------------------------------

<a
href="https://github.com/anubhavmaity/MiniVecDB/blob/main/minivecdb/ivf_index.py#L50"
target="_blank" style="float:right; font-size:smaller">source</a>

### IVFIndex

>  IVFIndex (storage:minivecdb.vector_storage.VectorStorage, nprobe=10,
>                distance_fn='cosine')

*Initialize self. See help(type(self)) for accurate signature.*

``` python
index = IVFIndex(vs, nprobe=10, distance_fn="cosine")
index.build()
index.get_stats()
```

    Average list size: 9.8, Max: 28

    {'built': True,
     'n_clusters': 39,
     'n_vectors': 383,
     'avg_cluster_size': 9.820512820512821,
     'min_cluster_size': 1,
     'max_cluster_size': 28,
     'empty_clusters': 0,
     'nprobe': 10}

``` python
query = "John Korir Claims Victory in Boston Marathon as Evans Lokedi Breaks Women's Course Record"
query_vector = gen_emb.get_embedding(query)
results = index.search(query_vector, k=5)
results.itemgot(1)
```

    (#5) [{'headline': 'Marathon Runner Breaks World Record', 'genre': 'Sports'},{'headline': 'Sprinter Breaks National Record', 'genre': 'Sports'},{'headline': 'Climber Sets New Speed Record', 'genre': 'Sports'},{'headline': 'City Hosts Annual Marathon', 'genre': 'Local News'},{'headline': 'Track Star Qualifies for World Championships', 'genre': 'Sports'}]

## Example Search Results

When we search with a query about marathons and records, we get highly
relevant results from our headline dataset - all about sports records
and running events.

This demonstrates how semantic search works - finding results based on
meaning, not just keywords.

## Adding New Vectors

The index also allows adding new vectors. When adding: 1. We store the
vectors in our VectorStorage 2. We rebuild the index to include these
new vectors

In a production system, you might use more efficient approaches for
updates.

``` python
news_headlines = fc.L("Dow Jumps 1,000 Points Tuesday as Markets Rebound from Recent Losses", # Finance
                  "Ronald Acuña Jr. Weeks Away from Return Following Knee Surgery", # Sports
                  "George Clooney Makes Broadway Debut Playing CBS News Legend Edward R. Murrow" # Entertainment
                )
```

``` python
embeddings = news_headlines.map(lambda o: gen_emb.get_embedding(o))
```

``` python
metadata = [{"headline": news_headlines[0], "genre": "finance"}, 
            {"headline": news_headlines[1], "genre": "sports"}, 
            {"headline": news_headlines[2], "genre": "entertainment"}]
index.add_vectors_bulk(embeddings, metadata)

stats = index.get_stats()
print(stats)
```

    Average list size: 9.8, Max: 28
    {'built': True, 'n_clusters': 39, 'n_vectors': 383, 'avg_cluster_size': 9.820512820512821, 'min_cluster_size': 1, 'max_cluster_size': 28, 'empty_clusters': 0, 'nprobe': 10}

``` python
index.search(query_vector, k=100).itemgot(1).attrgot('headline')
```

    (#100) ['Marathon Runner Breaks World Record','Sprinter Breaks National Record','Climber Sets New Speed Record','City Hosts Annual Marathon','Track Star Qualifies for World Championships','Skier Wins World Championship','Pole Vaulter Sets New Record','Boxer Wins Title in Stunning Knockout','Triathlete Wins National Championship','Swimmer Sets New Olympic Record','Boxer Wins Title in Final Round','Diver Sets New Depth Record','Rowing Team Wins National Title','Skier Wins Gold in World Cup','Cricket Team Secures Historic Victory','Weightlifter Sets New Personal Best','Surfer Wins World Championship','Kayaker Navigates Record-Breaking Rapids','Rugby Team Wins International Tournament','Swimmer Breaks National Record'...]

## Key Takeaways

The IVF Index gives us fast approximate nearest-neighbor search:

- **Speed**: Only search a fraction of vectors
- **Accuracy**: Usually finds the same results as an exhaustive search
- **Trade-offs**: Control speed vs accuracy with `nprobe`

This approach scales well to larger datasets, making vector search
practical for real applications.

Next, see 02_gen_embeddings.ipynb to learn how we create vectors from
text.
