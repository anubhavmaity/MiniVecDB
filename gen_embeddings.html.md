# Generating Embeddings with Gemini


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

## Setting Up the Client

First, we import the necessary libraries and set up our client.

Make sure you have an API key for Gemini stored in your environment
variables as ‘GEMINI_API_KEY’.

------------------------------------------------------------------------

<a
href="https://github.com/anubhavmaity/MiniVecDB/blob/main/minivecdb/gen_emb.py#L14"
target="_blank" style="float:right; font-size:smaller">source</a>

### get_embedding

>  get_embedding (contents, model='text-embedding-004')

## The Embedding Function

Our
[`get_embedding`](https://anubhavmaity.github.io/MiniVecDB/gen_embeddings.html#get_embedding)
function: 1. Takes text content as input 2. Sends it to Gemini’s
embedding API 3. Gets back a 768-dimensional vector 4. Returns it as a
numpy array

We’re using the “SEMANTIC_SIMILARITY” task type, which optimizes the
embeddings for finding related content.

The resulting vectors can be compared using cosine similarity or other
distance metrics to find similar content.

``` python
get_embedding("Dow Jumps 1,000 Points Tuesday as Markets Rebound from Recent Losses").shape
```

    (768,)

## Testing Our Function

Let’s test with a simple headline. The resulting embedding is a
768-dimensional vector.

Each dimension in this vector represents some aspect of the text’s
meaning, learned by the model during training. The specific meaning of
each dimension isn’t interpretable by humans, but the overall pattern of
values captures the semantic content.

We can use these embeddings to: - Find similar content - Group related
items - Build recommendation systems - Enable semantic search

Next, see 03_headline_embeddings.ipynb for a complete example of using
these embeddings.
